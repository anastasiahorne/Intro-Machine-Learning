{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSCI 470 Activities and Case Studies\n",
    "\n",
    "1. For all activities, you are allowed to collaborate with a partner. \n",
    "1. For case studies, you should work individually and are **not** allowed to collaborate.\n",
    "\n",
    "By filling out this notebook and submitting it, you acknowledge that you are aware of the above policies and are agreeing to comply with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some considerations with regard to how these notebooks will be graded:\n",
    "\n",
    "1. Cells in which \"# YOUR CODE HERE\" is found are the cells where your graded code should be written.\n",
    "2. In order to test out or debug your code you may also create notebook cells or edit existing notebook cells other than \"# YOUR CODE HERE\". We actually highly recommend you do so to gain a better understanding of what is happening. However, during grading, **these changes are ignored**. \n",
    "2. You must ensure that all your code for the particular task is available in the cells that say \"# YOUR CODE HERE\"\n",
    "3. Every cell that says \"# YOUR CODE HERE\" is followed by a \"raise NotImplementedError\". You need to remove that line. During grading, if an error occurs then you will not receive points for your work in that section.\n",
    "4. If your code passes the \"assert\" statements, then no output will result. If your code fails the \"assert\" statements, you will get an \"AssertionError\". Getting an assertion error means you will not receive points for that particular task.\n",
    "5. If you edit the \"assert\" statements to make your code pass, they will still fail when they are graded since the \"assert\" statements will revert to the original. Make sure you don't edit the assert statements.\n",
    "6. We may sometimes have \"hidden\" tests for grading. This means that passing the visible \"assert\" statements is not sufficient. The \"assert\" statements are there as a guide but you need to make sure you understand what you're required to do and ensure that you are doing it correctly. Passing the visible tests is necessary but not sufficient to get the grade for that cell.\n",
    "7. When you are asked to define a function, make sure you **don't** use any variables outside of the parameters passed to the function. You can think of the parameters being passed to the function as a hint. Make sure you're using all of those variables.\n",
    "8. Finally, **make sure you run \"Kernel > Restart and Run All\"** and pass all the asserts before submitting. If you don't restart the kernel, there may be some code that you ran and deleted that is still being used and that was why your asserts were passing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7ff8992190e52386",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\anast\\anaconda3\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\anast\\anaconda3\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\anast\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\anast\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\anast\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "# # Uncomment the below line to install\n",
    "#! pip install pprint\n",
    "#! pip install spacy\n",
    "#! python -m spacy download en_core_web_md\n",
    "#! pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-839fb7e9db39b4dd",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, HashingVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "import en_core_web_md\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and examine the data\n",
    "\n",
    "The \"20 newsgroups\" dataset contains message posts (\"documents\") from newgroup message boards, for 20 different topics.\n",
    "\n",
    "The ultimate goal of the models you construct will be to predict which topic a message belongs to. In order to accomplish these, you'll need to convert the text messages into numerical features, using the various methods we discussed in class. After converting the messages to numeric features, you'll train and test Naive Bayes and SVM models to perform topic classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-32547bf8b3c17d81",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(subset=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-222fb692deae0d18",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _20newsgroups_dataset:\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for training (or development)\n",
      "and the other one for testing (or for performance evaluation). The split\n",
      "between the train and test set is based upon a messages posted before\n",
      "and after a specific date.\n",
      "\n",
      "This module contains two loaders. The first one,\n",
      ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
      "returns a list of the raw texts that can be fed to text feature\n",
      "extractors such as :class:`~sklearn.feature_extraction.text.CountVectorizer`\n",
      "with custom parameters so as to extract feature vectors.\n",
      "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
      "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
      "extractor.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   ==========\n",
      "    Classes                     20\n",
      "    Samples total            18846\n",
      "    Dimensionality               1\n",
      "    Features                  text\n",
      "    =================   ==========\n",
      "\n",
      "Usage\n",
      "~~~~~\n",
      "\n",
      "The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n",
      "fetching / caching functions that downloads the data archive from\n",
      "the original `20 newsgroups website`_, extracts the archive contents\n",
      "in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",
      ":func:`sklearn.datasets.load_files` on either the training or\n",
      "testing set folder, or both of them::\n",
      "\n",
      "  >>> from sklearn.datasets import fetch_20newsgroups\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
      "\n",
      "  >>> from pprint import pprint\n",
      "  >>> pprint(list(newsgroups_train.target_names))\n",
      "  ['alt.atheism',\n",
      "   'comp.graphics',\n",
      "   'comp.os.ms-windows.misc',\n",
      "   'comp.sys.ibm.pc.hardware',\n",
      "   'comp.sys.mac.hardware',\n",
      "   'comp.windows.x',\n",
      "   'misc.forsale',\n",
      "   'rec.autos',\n",
      "   'rec.motorcycles',\n",
      "   'rec.sport.baseball',\n",
      "   'rec.sport.hockey',\n",
      "   'sci.crypt',\n",
      "   'sci.electronics',\n",
      "   'sci.med',\n",
      "   'sci.space',\n",
      "   'soc.religion.christian',\n",
      "   'talk.politics.guns',\n",
      "   'talk.politics.mideast',\n",
      "   'talk.politics.misc',\n",
      "   'talk.religion.misc']\n",
      "\n",
      "The real data lies in the ``filenames`` and ``target`` attributes. The target\n",
      "attribute is the integer index of the category::\n",
      "\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n",
      "\n",
      "It is possible to load only a sub-selection of the categories by passing the\n",
      "list of the categories to load to the\n",
      ":func:`sklearn.datasets.fetch_20newsgroups` function::\n",
      "\n",
      "  >>> cats = ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
      "\n",
      "  >>> list(newsgroups_train.target_names)\n",
      "  ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "\n",
      "Converting text to vectors\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "In order to feed predictive or clustering models with the text data,\n",
      "one first need to turn the text into vectors of numerical values suitable\n",
      "for statistical analysis. This can be achieved with the utilities of the\n",
      "``sklearn.feature_extraction.text`` as demonstrated in the following\n",
      "example that extract `TF-IDF`_ vectors of unigram tokens\n",
      "from a subset of 20news::\n",
      "\n",
      "  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "  >>> categories = ['alt.atheism', 'talk.religion.misc',\n",
      "  ...               'comp.graphics', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectorizer = TfidfVectorizer()\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> vectors.shape\n",
      "  (2034, 34118)\n",
      "\n",
      "The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n",
      "components by sample in a more than 30000-dimensional space\n",
      "(less than .5% non-zero features)::\n",
      "\n",
      "  >>> vectors.nnz / float(vectors.shape[0])\n",
      "  159.01327...\n",
      "\n",
      ":func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which\n",
      "returns ready-to-use token counts features instead of file names.\n",
      "\n",
      ".. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\n",
      ".. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\n",
      "\n",
      "\n",
      "Filtering text for more realistic training\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "It is easy for a classifier to overfit on particular things that appear in the\n",
      "20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n",
      "high F-scores, but their results would not generalize to other documents that\n",
      "aren't from this window of time.\n",
      "\n",
      "For example, let's look at the results of a multinomial Naive Bayes classifier,\n",
      "which is fast to train and achieves a decent F-score::\n",
      "\n",
      "  >>> from sklearn.naive_bayes import MultinomialNB\n",
      "  >>> from sklearn import metrics\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "  0.88213...\n",
      "\n",
      "(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n",
      "the training and test data, instead of segmenting by time, and in that case\n",
      "multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n",
      "yet of what's going on inside this classifier?)\n",
      "\n",
      "Let's take a look at what the most informative features are:\n",
      "\n",
      "  >>> import numpy as np\n",
      "  >>> def show_top10(classifier, vectorizer, categories):\n",
      "  ...     feature_names = vectorizer.get_feature_names_out()\n",
      "  ...     for i, category in enumerate(categories):\n",
      "  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
      "  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
      "  ...\n",
      "  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
      "  alt.atheism: edu it and in you that is of to the\n",
      "  comp.graphics: edu in graphics it is for and of to the\n",
      "  sci.space: edu it that is in and space to of the\n",
      "  talk.religion.misc: not it you in is that and to of the\n",
      "\n",
      "\n",
      "You can now see many things that these features have overfit to:\n",
      "\n",
      "- Almost every group is distinguished by whether headers such as\n",
      "  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n",
      "- Another significant feature involves whether the sender is affiliated with\n",
      "  a university, as indicated either by their headers or their signature.\n",
      "- The word \"article\" is a significant feature, based on how often people quote\n",
      "  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n",
      "  wrote:\"\n",
      "- Other features match the names and e-mail addresses of particular people who\n",
      "  were posting at the time.\n",
      "\n",
      "With such an abundance of clues that distinguish newsgroups, the classifiers\n",
      "barely have to identify topics from text at all, and they all perform at the\n",
      "same high level.\n",
      "\n",
      "For this reason, the functions that load 20 Newsgroups data provide a\n",
      "parameter called **remove**, telling it what kinds of information to strip out\n",
      "of each file. **remove** should be a tuple containing any subset of\n",
      "``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n",
      "blocks, and quotation blocks respectively.\n",
      "\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')\n",
      "  0.77310...\n",
      "\n",
      "This classifier lost over a lot of its F-score, just because we removed\n",
      "metadata that has little to do with topic classification.\n",
      "It loses even more if we also strip this metadata from the training data:\n",
      "\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "  0.76995...\n",
      "\n",
      "Some other classifiers cope better with this harder version of the task. Try the\n",
      ":ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py`\n",
      "example with and without the `remove` option to compare the results.\n",
      "\n",
      ".. topic:: Data Considerations\n",
      "\n",
      "  The Cleveland Indians is a major league baseball team based in Cleveland,\n",
      "  Ohio, USA. In December 2020, it was reported that \"After several months of\n",
      "  discussion sparked by the death of George Floyd and a national reckoning over\n",
      "  race and colonialism, the Cleveland Indians have decided to change their\n",
      "  name.\" Team owner Paul Dolan \"did make it clear that the team will not make\n",
      "  its informal nickname -- the Tribe -- its new team name.\" \"It's not going to\n",
      "  be a half-step away from the Indians,\" Dolan said.\"We will not have a Native\n",
      "  American-themed name.\"\n",
      "\n",
      "  https://www.mlb.com/news/cleveland-indians-team-name-change\n",
      "\n",
      ".. topic:: Recommendation\n",
      "\n",
      "  - When evaluating text classifiers on the 20 Newsgroups data, you\n",
      "    should strip newsgroup-related metadata. In scikit-learn, you can do this\n",
      "    by setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n",
      "    lower because it is more realistic.\n",
      "  - This text dataset contains data which may be inappropriate for certain NLP\n",
      "    applications. An example is listed in the \"Data Considerations\" section\n",
      "    above. The challenge with using current text datasets in NLP for tasks such\n",
      "    as sentence completion, clustering, and other applications is that text\n",
      "    that is culturally biased and inflammatory will propagate biases. This\n",
      "    should be taken into consideration when using the dataset, reviewing the\n",
      "    output, and the bias should be documented.\n",
      "\n",
      ".. topic:: Examples\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py`\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c1f44b15459bea17",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are the 20 topics that a message (\"document\") can belong to:\n",
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# Extract the messages and topic labels, and view the topic labels\n",
    "\n",
    "text = data[\"data\"]\n",
    "target = data[\"target\"]\n",
    "print('The following are the 20 topics that a message (\"document\") can belong to:')\n",
    "pprint(data[\"target_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample with label number \"10\", rec.sport.hockey\n",
      "\n",
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's look at an example data sample\n",
    "i_sample = 0\n",
    "print(f'A sample with label number \"{target[i_sample]}\", {data[\"target_names\"][target[i_sample]]}')\n",
    "print('')\n",
    "print(text[i_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2377d43a03f72b0f",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset contains 14134 messages.\n",
      "The test dataset contains 4712 messages.\n"
     ]
    }
   ],
   "source": [
    "# This \"20 newsgroups\" dataset has a pre-set train/test split, but in this\n",
    "# assignment we've loaded all the data, and will use a random shuffling and\n",
    "# split of the data, as we typically do.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text, target, random_state=0)\n",
    "\n",
    "print(f\"The training dataset contains {len(X_train)} messages.\")\n",
    "print(f\"The test dataset contains {len(X_test)} messages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create feature representations\n",
    "\n",
    "Below you will create numerical representations (a vectors) for each message using three methods we discussed in class.\n",
    "1. Bag of Words (BoW)\n",
    "2. Term frequency - Inverse Document Frequency (TF-IDF)\n",
    "3. Hashing\n",
    "\n",
    "Note that TF-IDF representations build upon the BoW representations by scaling the term counts in the Bag of Words document term matrix.\n",
    "\n",
    "Scikit-learn implements the BoW feature representation using [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), and it also has implementations for [TF-IDF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) and [hashed vector](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer) representations. Determine the feature representations of our dataset using each of those approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a548affddacf8b2bdadbec20f1d420c",
     "grade": false,
     "grade_id": "cell-f899f3942f01be93",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 9.42 s\n",
      "Wall time: 18.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Use \"english\" stopwords and produce a BoW representation for the data using n-grams with\n",
    "# n up to 3 (that is, use unigrams, bigrams, and trigrams).\n",
    "#\n",
    "# Save the vectorizer (transformer) as \"counter\".\n",
    "# Save the transformed data as \"X_train_bow\", and \"X_test_bow\".\n",
    "#\n",
    "# \"Fit\" the BoW transformer using the training data only. During the fitting process\n",
    "# the transformer identifies/learns its unique set of takens/features. When never before\n",
    "# seen tokens are found in the test set (or any other non-training data), they are\n",
    "# ignored. That is, they are not counted and included in the BoW representation matrix.\n",
    "#\n",
    "# Note that the CountVectorizer object is performing some of the text preprocessing\n",
    "# discussed in class, including tokenization and removing stop words (but not lemmatization).\n",
    "# The number of unique token occurences in each document (message) is done after\n",
    "# this preprocessing.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "counter = CountVectorizer(stop_words='english', ngram_range=(1, 3))\n",
    "X_train_bow = counter.fit_transform(X_train) \n",
    "X_test_bow = counter.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set has: 14134 samples and 3034327 features.\n",
      "The test set has:      4712 samples and 3034327 features.\n"
     ]
    }
   ],
   "source": [
    "# Let's see how many features (tokens) are in the BoW representations.\n",
    "# It will be a very large number (> 3M) owing to the use of n-grams with n up\n",
    "# to 3. It is also large to do the many typos, misspellings, or other\n",
    "# character sequences that are not actual english words, and thus treated\n",
    "# as a new/unique token.\n",
    "#\n",
    "# You may want to alter the cell above to use only unigrams, or\n",
    "# only unigrams and bigrams, and see how many features are created in\n",
    "# those situations. But use n-grams with n up to three before moving\n",
    "# forward in the notebook.\n",
    "\n",
    "print(f'The training set has: {X_train_bow.shape[0]:5d} samples and {X_train_bow.shape[1]} features.')\n",
    "print(f'The test set has:     {X_test_bow.shape[0]:5d} samples and {X_test_bow.shape[1]} features.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c4e2d2a908a09be0726df3099868d81",
     "grade": true,
     "grade_id": "cell-740433b073b13510",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m counter\u001b[38;5;241m.\u001b[39mstop_words\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m counter\u001b[38;5;241m.\u001b[39mngram_range\u001b[38;5;241m==\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(counter\u001b[38;5;241m.\u001b[39mget_feature_names())\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m3034327\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m X_train_bow\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m==\u001b[39m(\u001b[38;5;241m14134\u001b[39m, \u001b[38;5;241m3034327\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m X_test_bow\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m==\u001b[39m(\u001b[38;5;241m4712\u001b[39m, \u001b[38;5;241m3034327\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "assert counter\n",
    "assert counter.stop_words==\"english\"\n",
    "assert counter.ngram_range==(1, 3)\n",
    "assert len(counter.get_feature_names())==3034327\n",
    "assert X_train_bow.shape==(14134, 3034327)\n",
    "assert X_test_bow.shape==(4712, 3034327)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term frequency - Inverse document frequency\n",
    "\n",
    "Note that sklearn implements a [`TfidfVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) and [`TfidfTransformer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html). The main difference between the two is in the inputs to `fit_transform` and `transform`. The [Vectorizer's fit/transform](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.fit) take an input of text whereas the [Transformer's](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer.fit) take an input of a BoW matrix. Given that we already determined the BoW matrix, it would be more time efficient to use `TfidfTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3414772ac5b3108202d8dcfca809f42",
     "grade": false,
     "grade_id": "cell-018bdbbae2613aef",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 812 ms\n",
      "Wall time: 1.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Use the BoW representation you just created above to produce a TF-IDF representation of the data\n",
    "#\n",
    "# Save the transformer to \"tfidfer\".\n",
    "# Save the transformed data as \"X_train_tfidf\", and \"X_test_tfidf\".\n",
    "#\n",
    "# As with BoW, using only training data representations for fitting, during\n",
    "# which time the TF-IDF transformer determines the fixed set of tokens that\n",
    "# it will represent.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "tfidfer = TfidfTransformer()\n",
    "X_train_tfidf = tfidfer.fit_transform(X_train_bow)\n",
    "X_test_tfidf = tfidfer.transform(X_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "002c4691b85e923f64b114fd54088c1f",
     "grade": true,
     "grade_id": "cell-0e6ce5999cd93389",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert tfidfer\n",
    "assert X_train_tfidf.shape==(14134, 3034327)\n",
    "assert X_test_tfidf.shape==(4712, 3034327)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ad711b8f3b748c9fd4a1ff2a0ae7053",
     "grade": false,
     "grade_id": "cell-ed956dd2a828dea8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Hashing vectorizer\n",
    "\n",
    "So far, we've \"vectorized\" the raw text messages into numerical feature vectors (assembled into a matrix) using BoW and TF-IDF. Both of those techniques determine a vocabulary, a fixed number of tokens, from the training data. When new tokens are found in the test set data they are ignored.\n",
    "\n",
    "The hashing vectorizer remedies (but not without a downside) this situation by using a hash algorithm to convert a text token into an index (location) into the representation vector. A hash algorithm converts any sequence of bits into a bit sequence of fixed length. That fixed length sequence is thus an integer. There are many facets to this, but as this is not a cryptogryphy course (hashing is often used as a method to confirm that a digital document/stream has not been altered), we won't discuss those facets here.\n",
    "\n",
    "The main point is, a hashing vectorizer can deal with test set tokens that is has never seen before, and \"find\" a place for them in the output feature representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b420684464adae5a7a1fa7ce6acadad3",
     "grade": false,
     "grade_id": "cell-2ad00d9b9df56ed5",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.98 s\n",
      "Wall time: 6.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Use \"english\" stopwords and produce a hashed vector representation for the data using up to trigrams.\n",
    "#\n",
    "# Save the vectorizer as \"hasher\".\n",
    "# Save the transformed data as \"X_train_hash\" and \"X_test_hash\".\n",
    "#\n",
    "# Make sure you set alternate_sign to False so we can use this representation with Multinomial Naive\n",
    "# Bayes later in the notebook.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "hasher = HashingVectorizer(stop_words='english', ngram_range=(1, 3), alternate_sign=False)\n",
    "X_train_hash = hasher.transform(X_train)\n",
    "X_test_hash = hasher.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set has: 14134 samples and 1048576 features.\n",
      "The test set has:      4712 samples and 1048576 features.\n"
     ]
    }
   ],
   "source": [
    "# Let's see how many features (tokens) are in the hashed representations\n",
    "\n",
    "print(f'The training set has: {X_train_hash.shape[0]:5d} samples and {X_train_hash.shape[1]} features.')\n",
    "print(f'The test set has:     {X_test_hash.shape[0]:5d} samples and {X_test_hash.shape[1]} features.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fee5c79f5236e12894fdf52279d3f9a",
     "grade": true,
     "grade_id": "cell-f6a5afb5d4f0a4d9",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert hasher\n",
    "assert hasher.stop_words == \"english\"\n",
    "assert hasher.ngram_range == (1,3)\n",
    "assert X_train_hash.shape == (14134, 1048576)\n",
    "assert X_test_hash.shape == (4712, 1048576)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having passed the asserts above, we see that the hashing vectorizer produced feature vectors with 1048576 features. Recall that the hash algorithm creates outputs with a fixed number of bits, and `2**n_bits` must be the number of possible features in the representation.\n",
    "\n",
    "Just out of curiosity, let's see how many output bits were produced by the hash algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hash algorithm generated 20.0-bit representations, and thus 2^20.0==1048576 features.\n"
     ]
    }
   ],
   "source": [
    "# Just out of curiosity, let's see how many output bits were produced by the hash algorithm.\n",
    "\n",
    "n_features = X_test_hash.shape[1]\n",
    "n_bits = np.log2(n_features)\n",
    "print(f\"The hash algorithm generated {n_bits}-bit representations, and thus 2^{n_bits}=={n_features} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the time it took to run the count vectorizer versus the hashing vectorizer even though they both will iterate through all the words.\n",
    "\n",
    "Note that TF-IDF built upon the BoW representations, to that BoW time should be added in to the TF-IDF time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Naive Bayes classifier model, with engineered features\n",
    "\n",
    "New you'll build classifier models that use the vector feature representations you just created.\n",
    "\n",
    "Recall [Naive Bayes Classification](http://scikit-learn.org/stable/modules/naive_bayes.html) which we discussed early on in the supervised learning lectures. We will use Naive Bayes classifiers to predict the topic of the articles and compare our feature representations.\n",
    "\n",
    "Use a Multinomial Naive Bayes classifier to predict the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf8ba8d7e6d600d38c11be1dd5c1109f",
     "grade": false,
     "grade_id": "cell-cdf5ba26c387fd97",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "_validate_params not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feat_name, train_feat, test_feat \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBag of Words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF-IDF\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHashing\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      2\u001b[0m                                             [X_train_bow, X_train_tfidf, X_train_hash],\n\u001b[0;32m      3\u001b[0m                                             [X_test_bow, X_test_tfidf, X_test_hash]):\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# the BoW, TF-IDF, or hashing features in this loop.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     mnb\u001b[38;5;241m=\u001b[39mMultinomialNB\u001b[38;5;241m.\u001b[39mfit(train_feat, y_train) \n\u001b[0;32m     11\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m mnb\u001b[38;5;241m.\u001b[39mpredict(test_feat)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeat_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1144\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1140\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1141\u001b[0m )\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_base.py:771\u001b[0m, in \u001b[0;36mspmatrix.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetnnz()\n\u001b[0;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(attr \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: _validate_params not found"
     ]
    }
   ],
   "source": [
    "for feat_name, train_feat, test_feat in zip([\"Bag of Words\", \"TF-IDF\", \"Hashing\"],\n",
    "                                            [X_train_bow, X_train_tfidf, X_train_hash],\n",
    "                                            [X_test_bow, X_test_tfidf, X_test_hash]):\n",
    "    \n",
    "    # Create a Multinomial Naive Bayes model and saved it to `mnb`\n",
    "    # Fit the 'mnb' model to the training features and labels, for\n",
    "    # the BoW, TF-IDF, or hashing features in this loop.\n",
    "    # YOUR CODE HERE\n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(train_feat, y_train) \n",
    "    \n",
    "    y_pred = mnb.predict(test_feat)\n",
    "    print(f\"Results for {feat_name}\")\n",
    "    print(\"-\"*80)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c6868948856264f42927eef81e550203",
     "grade": true,
     "grade_id": "cell-b545c8d3793a05e7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(mnb, MultinomialNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (optional)\n",
    "\n",
    "LDA is a topic modeling approach that can be used to determine high level topics covered in a corpus. Identified topics can help us determine the concepts covered in the corpus and better understand what is being discussed. The topic relevance can be used as a useful feature representation for each document. \n",
    "\n",
    "Scikit-learn has an implementation available for [LDA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html).\n",
    "\n",
    "Fitting this data can take a **very long time** so this code is just provided for you to uncomment it and examine the results. You can continue with the rest of the notebook while this runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Feel free to change the number of topics to find by updating n_components\n",
    "# lda = LatentDirichletAllocation(n_components=30)\n",
    "# lda.fit(X_train_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    rows = model.n_components // 5\n",
    "    if rows % 5 != 0:\n",
    "        rows +=1\n",
    "    fig, axes = plt.subplots(rows, 5, figsize=(45, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f'Topic {topic_idx +1}',\n",
    "                     fontdict={'fontsize': 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        for i in 'top right left'.split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_top_words(lda, counter.get_feature_names(), 10, \"Topic Content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The following names are the target classes:\")\n",
    "print(data.target_names)\n",
    "print(f\"What do you think of the similarity between topics you found and the classes? Are these useful topics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# X_train_lda = lda.transform(X_train_bow)\n",
    "# X_test_lda = lda.transform(X_test_bow)\n",
    "# svm = LinearSVC().fit(X_train_lda, y_train)\n",
    "# y_pred = svm.predict(X_test_lda)\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PLEASE**  \n",
    "\n",
    "**If you uncommented and ran the above Latent Dirichlet Allocation (LDA) code, recomment it afterwards, as this will help speed up grading time.**  \n",
    "\n",
    "**Thank you!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c96453fa3ea624ebe11c4066e3785057",
     "grade": false,
     "grade_id": "cell-f1fb206ff47bb7a6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Learned Embeddings\n",
    "\n",
    "We'll now move on to the use of __learned__ representations, rather than ones the were, to some degree, engineered (e.g., the decision to base the features on token counts). We will use [spacy](https://spacy.io/), for more sophisticated NLP. Make sure you downloaded the english model in the commented code at the top of the notebook (`en_core_web_md`) before proceeding. It may take some time to download.\n",
    "\n",
    "Spacy allows us to parse text and automatically does the following:\n",
    "- tokenization\n",
    "- lemmatization\n",
    "- sentence splitting\n",
    "- entity recognition\n",
    "- token vector representation\n",
    "\n",
    "\n",
    "__We'll start by creating an example string, and observing the results of `spacy`'s preprocessing. Then you can create your own example string and do the same.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bdd9ef5d3e965340",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 938 ms\n",
      "Wall time: 1.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# The spacy models are of small (sm), medium (md) and large (lg)\n",
    "# sizes. We'll use the medium-sized model.\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d4194dd187aa95b1",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing sentence: This is the first sentence in this test string.\n",
      "Lemmatization: this be the first sentence in this test string.\n",
      "Analyzing token: This\n",
      "This token is the first one in the sentence\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: PRON\n",
      "Lemma: this\n",
      "----------\n",
      "Analyzing token: is\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: AUX\n",
      "Lemma: be\n",
      "----------\n",
      "Analyzing token: the\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: the\n",
      "----------\n",
      "Analyzing token: first\n",
      "Stop word\n",
      "Entity type: ORDINAL\n",
      "Part of speech: ADJ\n",
      "Lemma: first\n",
      "----------\n",
      "Analyzing token: sentence\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: sentence\n",
      "----------\n",
      "Analyzing token: in\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADP\n",
      "Lemma: in\n",
      "----------\n",
      "Analyzing token: this\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: this\n",
      "----------\n",
      "Analyzing token: test\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: test\n",
      "----------\n",
      "Analyzing token: string\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: string\n",
      "----------\n",
      "Analyzing token: .\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: .\n",
      "----------\n",
      "--------------------------------------------------\n",
      "Analyzing sentence: The quick brown fox jumps over the lazy dog.\n",
      "Lemmatization: the quick brown fox jump over the lazy dog.\n",
      "Analyzing token: The\n",
      "This token is the first one in the sentence\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: the\n",
      "----------\n",
      "Analyzing token: quick\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: ADJ\n",
      "Lemma: quick\n",
      "----------\n",
      "Analyzing token: brown\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: ADJ\n",
      "Lemma: brown\n",
      "----------\n",
      "Analyzing token: fox\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: fox\n",
      "----------\n",
      "Analyzing token: jumps\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: jump\n",
      "----------\n",
      "Analyzing token: over\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADP\n",
      "Lemma: over\n",
      "----------\n",
      "Analyzing token: the\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: the\n",
      "----------\n",
      "Analyzing token: lazy\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: ADJ\n",
      "Lemma: lazy\n",
      "----------\n",
      "Analyzing token: dog\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: dog\n",
      "----------\n",
      "Analyzing token: .\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: .\n",
      "----------\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"This is the first sentence in this test string. The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "parsed_text = nlp(text)\n",
    "\n",
    "for sent in parsed_text.sents:\n",
    "    print(f\"Analyzing sentence: {sent}\")\n",
    "    print(f\"Lemmatization: {sent.lemma_}\")\n",
    "    for token in sent:\n",
    "        print(f\"Analyzing token: {token}\")\n",
    "        if token.is_sent_start:\n",
    "            print(\"This token is the first one in the sentence\")\n",
    "        if token.is_stop:\n",
    "            print(\"Stop word\")\n",
    "        else:\n",
    "            print(\"Not stop word\")\n",
    "        print(f\"Entity type: {token.ent_type_}\")\n",
    "        print(f\"Part of speech: {token.pos_}\")\n",
    "        print(f\"Lemma: {token.lemma_}\")\n",
    "        print(\"-\"*10)\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14751fcb1029c6fe4c1ea2b5336a5410",
     "grade": false,
     "grade_id": "cell-a1f85ff1f2b6b116",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Come up with a couple sentences to test out, put them into a single string (pair\n",
    "# of quuotes), and name that string \"my_text\".\n",
    "# Or, go to a website and copy a paragraph from there.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "my_text = \"Natural language processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language. It involves the development of algorithms and models to understand, interpret, and generate human language. NLP applications include chatbots, sentiment analysis, machine translation, and more.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80badb1bad0222f6ce477e3787c284bc",
     "grade": true,
     "grade_id": "cell-dd4c284f3101e2e6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(my_text) > 10\n",
    "assert my_text.count(\".\") > 2  # Two or more sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fd7fed9c7e3903f4",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing sentence: Natural language processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language.\n",
      "Lemmatization: natural language processing (NLP) be a subfield of artificial intelligence (AI) that focus on the interaction between computer and human through natural language.\n",
      "Analyzing token: Natural\n",
      "This token is the first one in the sentence\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: ADJ\n",
      "Lemma: natural\n",
      "----------\n",
      "Analyzing token: language\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: language\n",
      "----------\n",
      "Analyzing token: processing\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: processing\n",
      "----------\n",
      "Analyzing token: (\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: (\n",
      "----------\n",
      "Analyzing token: NLP\n",
      "Not stop word\n",
      "Entity type: ORG\n",
      "Part of speech: PROPN\n",
      "Lemma: NLP\n",
      "----------\n",
      "Analyzing token: )\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: )\n",
      "----------\n",
      "Analyzing token: is\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: AUX\n",
      "Lemma: be\n",
      "----------\n",
      "Analyzing token: a\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: a\n",
      "----------\n",
      "Analyzing token: subfield\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: subfield\n",
      "----------\n",
      "Analyzing token: of\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADP\n",
      "Lemma: of\n",
      "----------\n",
      "Analyzing token: artificial\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: ADJ\n",
      "Lemma: artificial\n",
      "----------\n",
      "Analyzing token: intelligence\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: intelligence\n",
      "----------\n",
      "Analyzing token: (\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: (\n",
      "----------\n",
      "Analyzing token: AI\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PROPN\n",
      "Lemma: AI\n",
      "----------\n",
      "Analyzing token: )\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: )\n",
      "----------\n",
      "Analyzing token: that\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: PRON\n",
      "Lemma: that\n",
      "----------\n",
      "Analyzing token: focuses\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: focus\n",
      "----------\n",
      "Analyzing token: on\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADP\n",
      "Lemma: on\n",
      "----------\n",
      "Analyzing token: the\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: the\n",
      "----------\n",
      "Analyzing token: interaction\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: interaction\n",
      "----------\n",
      "Analyzing token: between\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADP\n",
      "Lemma: between\n",
      "----------\n",
      "Analyzing token: computers\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: computer\n",
      "----------\n",
      "Analyzing token: and\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: CCONJ\n",
      "Lemma: and\n",
      "----------\n",
      "Analyzing token: humans\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: human\n",
      "----------\n",
      "Analyzing token: through\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADP\n",
      "Lemma: through\n",
      "----------\n",
      "Analyzing token: natural\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: ADJ\n",
      "Lemma: natural\n",
      "----------\n",
      "Analyzing token: language\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: language\n",
      "----------\n",
      "Analyzing token: .\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: .\n",
      "----------\n",
      "--------------------------------------------------\n",
      "Analyzing sentence: It involves the development of algorithms and models to understand, interpret, and generate human language.\n",
      "Lemmatization: it involve the development of algorithm and model to understand, interpret, and generate human language.\n",
      "Analyzing token: It\n",
      "This token is the first one in the sentence\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: PRON\n",
      "Lemma: it\n",
      "----------\n",
      "Analyzing token: involves\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: involve\n",
      "----------\n",
      "Analyzing token: the\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: the\n",
      "----------\n",
      "Analyzing token: development\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: development\n",
      "----------\n",
      "Analyzing token: of\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADP\n",
      "Lemma: of\n",
      "----------\n",
      "Analyzing token: algorithms\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: algorithm\n",
      "----------\n",
      "Analyzing token: and\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: CCONJ\n",
      "Lemma: and\n",
      "----------\n",
      "Analyzing token: models\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: model\n",
      "----------\n",
      "Analyzing token: to\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: PART\n",
      "Lemma: to\n",
      "----------\n",
      "Analyzing token: understand\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: understand\n",
      "----------\n",
      "Analyzing token: ,\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: ,\n",
      "----------\n",
      "Analyzing token: interpret\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: interpret\n",
      "----------\n",
      "Analyzing token: ,\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: ,\n",
      "----------\n",
      "Analyzing token: and\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: CCONJ\n",
      "Lemma: and\n",
      "----------\n",
      "Analyzing token: generate\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: generate\n",
      "----------\n",
      "Analyzing token: human\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: ADJ\n",
      "Lemma: human\n",
      "----------\n",
      "Analyzing token: language\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: language\n",
      "----------\n",
      "Analyzing token: .\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: .\n",
      "----------\n",
      "--------------------------------------------------\n",
      "Analyzing sentence: NLP applications include chatbots, sentiment analysis, machine translation, and more.\n",
      "Lemmatization: NLP application include chatbot, sentiment analysis, machine translation, and more.\n",
      "Analyzing token: NLP\n",
      "This token is the first one in the sentence\n",
      "Not stop word\n",
      "Entity type: ORG\n",
      "Part of speech: PROPN\n",
      "Lemma: NLP\n",
      "----------\n",
      "Analyzing token: applications\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: application\n",
      "----------\n",
      "Analyzing token: include\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: include\n",
      "----------\n",
      "Analyzing token: chatbots\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: chatbot\n",
      "----------\n",
      "Analyzing token: ,\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: ,\n",
      "----------\n",
      "Analyzing token: sentiment\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: sentiment\n",
      "----------\n",
      "Analyzing token: analysis\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: analysis\n",
      "----------\n",
      "Analyzing token: ,\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: ,\n",
      "----------\n",
      "Analyzing token: machine\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: machine\n",
      "----------\n",
      "Analyzing token: translation\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: translation\n",
      "----------\n",
      "Analyzing token: ,\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: ,\n",
      "----------\n",
      "Analyzing token: and\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: CCONJ\n",
      "Lemma: and\n",
      "----------\n",
      "Analyzing token: more\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADJ\n",
      "Lemma: more\n",
      "----------\n",
      "Analyzing token: .\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: .\n",
      "----------\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "parsed = nlp(my_text)\n",
    "for sent in parsed.sents:\n",
    "    print(f\"Analyzing sentence: {sent}\")\n",
    "    print(f\"Lemmatization: {sent.lemma_}\")\n",
    "    for token in sent:\n",
    "        print(f\"Analyzing token: {token}\")\n",
    "        if token.is_sent_start:\n",
    "            print(\"This token is the first one in the sentence\")\n",
    "        if token.is_stop:\n",
    "            print(\"Stop word\")\n",
    "        else:\n",
    "            print(\"Not stop word\")\n",
    "        print(f\"Entity type: {token.ent_type_}\")\n",
    "        print(f\"Part of speech: {token.pos_}\")\n",
    "        print(f\"Lemma: {token.lemma_}\")\n",
    "        print(\"-\"*10)\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "86ee98556c7ecd9eb1204436cd5f2ce6",
     "grade": false,
     "grade_id": "cell-1a2a413b9c440b95",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "If we use the larger `spacy` models, we'll get the __GloVe representation__ for some words. GloVe representations are \"pre-trained\" (learned) from a large language corpus. In this case, the GloVe vectors should have 300 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-da272ac031f8d791",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "915b7ab80adeb58c07190fd8ecd0d448",
     "grade": false,
     "grade_id": "cell-7d1247a2cf60bab3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## An SVM classifier model, with learned embedding features\n",
    "\n",
    "Now you'll use `spacy` to get GloVe feature representations of a subset of the messages, and then train and test an SVM that makes topic predictions based on those features.\n",
    "\n",
    "Given that the parsing of text takes some time, we will only use __the first 1000 messages__ in our data. __You will get notably lower performance versus the NB models which used the engineered features, but that is almost solely due to the use of a much smaller data set here.__ You may edit the cell below and use more data, but __please__ reset it to 1000 afterwards, or the autograder may not be able to score your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-65e1d1dbb5567b87",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "new_X_train, new_X_test, new_y_train, new_y_test = train_test_split(X_train[:1000], y_train[:1000], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aafc43301aa0ecfb078b324a43b32553",
     "grade": false,
     "grade_id": "cell-a35e8a4ea5c845f6",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Using the \"nlp\" from above, parse every instance of new_X_train\n",
    "# save the document vectors to a np.array called X_train_glove\n",
    "\n",
    "# YOUR CODE HERE\n",
    "X_train_glove = []\n",
    "\n",
    "for string in new_X_train:\n",
    "    parsed = nlp(string)\n",
    "    doc= parsed.vector\n",
    "    X_train_glove.append(doc)\n",
    "\n",
    "X_train_glove = np.array(X_train_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3cbbe22214eea789813b27bee43dc314",
     "grade": true,
     "grade_id": "cell-8431bb15ebe0914f",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert X_train_glove.shape == (len(new_X_train), 300)\n",
    "assert X_test_glove.shape == (len(new_X_test), 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5b693b9255ee6aea",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "svm = LinearSVC().fit(X_train_glove, new_y_train)\n",
    "y_pred = svm.predict(X_test_glove)\n",
    "\n",
    "# Due to the smaller data set size, you may get \"Precision and F-score are ill-defined\"\n",
    "# warnings from the classification_report() below. That is anticipated and of no concern.\n",
    "print(classification_report(new_y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "483e65acc4062f57f7320d9b4cb0f945",
     "grade": false,
     "grade_id": "cell-75181722913aa756",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed936ab53a1391c5e6af8df699a1dbf5",
     "grade": false,
     "grade_id": "feedback",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def feedback():\n",
    "    \"\"\"Provide feedback on the contents of this exercise\n",
    "    \n",
    "    Returns:\n",
    "        string\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "   return \"no feedback\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f39f6185a54850c2f1f9b5b2a17b7543",
     "grade": true,
     "grade_id": "feedback-tests",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
